{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: imageio[ffmpeg] in /home/hice1/arothe6/.local/lib/python3.10/site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (1.22.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (9.2.0)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Obtaining dependency information for imageio-ffmpeg from https://files.pythonhosted.org/packages/a0/2d/43c8522a2038e9d0e7dbdf3a61195ecc31ca576fb1527a528c877e87d973/imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (5.9.4)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m294.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/storage/ice1/5/4/arothe6/CS_7643_project\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %pip install imageio[ffmpeg]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from model1 import Model1\n",
    "from endovis_dataset import EndovisDataset\n",
    "from reformat_data import convert_all_data\n",
    "\n",
    "segmentation_model_weights_path = \"model1_weights.pth\"\n",
    "tracking_model_weights_path = \"tracking_model_weights.pth\"\n",
    "\n",
    "og_data_path = r\"original_data\"\n",
    "data_path = r\"data\"\n",
    "\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_data/segmentation_train/Training/Dataset2/Video1.avi\n",
      "original_data/segmentation_train/Training/Dataset2/Segmentation1.avi\n",
      "original_data/segmentation_train/Training/Dataset3/Video1.avi\n",
      "original_data/segmentation_train/Training/Dataset3/Segmentation1.avi\n",
      "original_data/segmentation_train/Training/Dataset1/Video1.avi\n",
      "original_data/segmentation_train/Training/Dataset1/Left_Instrument_Segmentation1.avi\n",
      "original_data/segmentation_train/Training/Dataset1/Right_Instrument_Segmentation1.avi\n",
      "original_data/segmentation_train/Training/Dataset4/Video1.avi\n",
      "original_data/segmentation_train/Training/Dataset4/Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset5/Video1.avi\n",
      "original_data/segmentation_test/Dataset5/Left_Instrument_Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset5/Right_Instrument_Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset2/Video1.avi\n",
      "original_data/segmentation_test/Dataset2/Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset3/Video1.avi\n",
      "original_data/segmentation_test/Dataset3/Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset6/Video1.avi\n",
      "original_data/segmentation_test/Dataset6/Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset1/Video1.avi\n",
      "original_data/segmentation_test/Dataset1/Left_Instrument_Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset1/Right_Instrument_Segmentation1.avi\n",
      "original_data/segmentation_test/Dataset4/Video1.avi\n",
      "original_data/segmentation_test/Dataset4/Segmentation1.avi\n",
      "original_data/tracking_train/Training/Dataset2/Video1.avi\n",
      "original_data/tracking_train/Training/Dataset3/Video1.avi\n",
      "original_data/tracking_train/Training/Dataset1/Video1.avi\n",
      "original_data/tracking_train/Training/Dataset4/Video1.avi\n",
      "original_data/tracking_test/Dataset5/Video1.avi\n",
      "original_data/tracking_test/Dataset2/Video1.avi\n",
      "original_data/tracking_test/Dataset3/Video1.avi\n",
      "original_data/tracking_test/Dataset6/Video1.avi\n",
      "original_data/tracking_test/Dataset1/Video1.avi\n",
      "original_data/tracking_test/Dataset4/Video1.avi\n"
     ]
    }
   ],
   "source": [
    "# Converts all the data from video to npz files\n",
    "# Only need to run this once when setting up your system\n",
    "# It will probably take 10 minutes to run; be patient :)\n",
    "\n",
    "# Subfolders where data is; True indicates it is segmentation data\n",
    "subfolders = { \n",
    "    \"segmentation_train\": True,\n",
    "    \"segmentation_test\": True,\n",
    "    \"tracking_train\": False,\n",
    "    \"tracking_test\": False\n",
    "}\n",
    "convert_all_data(og_data_path, data_path, subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel1\u001b[49m(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# Switch out with whichever model you like\u001b[39;00m\n\u001b[1;32m     13\u001b[0m folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m dataset \u001b[38;5;241m=\u001b[39m EndovisDataset(folder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model1' is not defined"
     ]
    }
   ],
   "source": [
    "# Training for Segmentation\n",
    "\n",
    "# Hyperparameters \n",
    "# TODO tune these! I just made them randomly\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "\n",
    "# Model\n",
    "model = Model1(4) # Switch out with whichever model you like\n",
    "\n",
    "folder = os.path.join(data_path, \"segmentation_train\")\n",
    "dataset = EndovisDataset(folder)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # TODO can tune batch size\n",
    "\n",
    "criterion = nn.BCELoss() # Because https://medium.com/@kitkat73275/multi-label-classification-8d8ae55e8373#:~:text=For%20multi%2Dlabel%20classification%20with,class%20labels%20for%20new%20instances.\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) # TODO tune these hyperparameters\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch %d:\" % epoch)\n",
    "    for frames, truths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, truths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Loss: %f\" % loss.item())\n",
    "\n",
    "torch.save(model.state_dict(), segmentation_model_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for Segmentation\n",
    "\n",
    "model = Model1()\n",
    "model.load_state_dict(torch.load(segmentation_model_weights_path))\n",
    "model.eval()\n",
    "\n",
    "test_folder = r\"data\\segmentation_test\"\n",
    "testset = EndovisDataset(test_folder)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for frames, truths in testloader:\n",
    "        outputs = torch.sigmoid(model(frames)) # Apply sigmoid for probabilities\n",
    "        # TODO Evaluate performance, like IOU or DICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for Tracking\n",
    "\n",
    "# TODO Cloud\n",
    "# Make a new model in a new file\n",
    "# Train it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for Tracking\n",
    "\n",
    "# TODO Cloud\n",
    "# Test the new model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
